{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69cc86f2",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ What are we building (learning-first mindset)\n",
    "\n",
    "This project will let you:\n",
    "\n",
    "* Safely store API keys\n",
    "* Call Groq LLMs\n",
    "* Experiment with prompts & parameters\n",
    "* Reuse code later for **Streamlit, agents, LangGraph**\n",
    "\n",
    "Think of this as your **LLM API playground**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Recommended project structure (clean & extensible)\n",
    "\n",
    "```\n",
    "groq-llm-playground/\n",
    "â”‚\n",
    "â”œâ”€â”€ .env                 # Secrets (API keys)\n",
    "â”œâ”€â”€ .gitignore           # Prevent secrets from leaking\n",
    "â”œâ”€â”€ requirements.txt     # Python dependencies\n",
    "â”œâ”€â”€ README.md            # Project notes & experiments\n",
    "â”‚\n",
    "â”œâ”€â”€ app/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ config.py        # Load env variables\n",
    "â”‚   â”œâ”€â”€ client.py        # Groq client initialization\n",
    "â”‚   â”œâ”€â”€ prompts.py       # Reusable prompt templates\n",
    "â”‚   â””â”€â”€ chat.py          # Chat completion logic\n",
    "â”‚\n",
    "â””â”€â”€ experiments/\n",
    "    â”œâ”€â”€ basic_chat.py    # First API call\n",
    "    â”œâ”€â”€ temperature_test.py\n",
    "    â”œâ”€â”€ streaming_demo.py\n",
    "```\n",
    "\n",
    "This is **not overengineering** â€” this is how real LLM apps are structured.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Why each file exists (important mental model)\n",
    "\n",
    "### ğŸ“„ `.env` (very important)\n",
    "\n",
    "Stores secrets safely.\n",
    "\n",
    "```\n",
    "GROQ_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "âœ” Never commit this\n",
    "âœ” Works locally & in deployment\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“„ `.gitignore`\n",
    "\n",
    "```\n",
    ".env\n",
    "__pycache__/\n",
    ".venv/\n",
    "```\n",
    "\n",
    "Prevents:\n",
    "\n",
    "* API key leaks\n",
    "* Virtual env clutter\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“„ `requirements.txt`\n",
    "\n",
    "```\n",
    "groq\n",
    "python-dotenv\n",
    "```\n",
    "\n",
    "Why `python-dotenv`?\n",
    "â†’ It loads `.env` into Python automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Core application files (app/)\n",
    "\n",
    "### ğŸ”¹ `config.py` â€“ environment loader\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ API not founbd in environment\")\n",
    "else :\n",
    "    print(\" GROQ API KEY is present we are ready to proceed further \")\n",
    "```\n",
    "\n",
    "ğŸ’¡ This fails early if your setup is wrong â€” very professional habit.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ `client.py` â€“ Groq client (single source of truth)\n",
    "\n",
    "```python\n",
    "from groq import Groq\n",
    "from app.config import GROQ_API_KEY\n",
    "\n",
    "def get_groq_client():\n",
    "    return Groq(api_key = GROQ_API_KEY)\n",
    "```\n",
    "\n",
    "âœ” Reusable\n",
    "âœ” Swappable later (OpenAI / local LLMs)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ `prompts.py` â€“ reusable prompt patterns\n",
    "\n",
    "```python\n",
    "SYSTEM_PROMPT = \"You are a helpful AI assistant .\"\n",
    "\n",
    "def build_chat_prompt(user_input:str):\n",
    "    return [\n",
    "        {\"role\": \"system\",\"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\"content\": user_input}\n",
    "    ]\n",
    "```\n",
    "\n",
    "This prepares you for:\n",
    "\n",
    "* Prompt engineering\n",
    "* Agent system prompts\n",
    "* Role-based control\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ `chat.py` â€“ core LLM interaction logic\n",
    "\n",
    "```python\n",
    "from app.client import get_groq_client\n",
    "from app.prompts import build_chat_prompt\n",
    "\n",
    "def chat_completion(\n",
    "    user_input: str,\n",
    "    model: str = \"openai/gpt-oss-120b\",\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 512\n",
    "):\n",
    "    client = get_groq_client()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=build_chat_prompt(user_input),\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "```\n",
    "\n",
    "This becomes your **LLM engine**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Experiments folder (learning without fear)\n",
    "\n",
    "### ğŸ”¬ `basic_chat.py`\n",
    "\n",
    "```python\n",
    "from app.chat import chat_completion\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reply = chat_completion(\"Explain self-attention in simple terms\")\n",
    "    print(reply)\n",
    "```\n",
    "\n",
    "Youâ€™ll:\n",
    "\n",
    "* Test ideas\n",
    "* Break things safely\n",
    "* Learn fast\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Why this structure is future-proof\n",
    "\n",
    "With this exact setup, you can later:\n",
    "\n",
    "* ğŸ”Œ Plug into **Streamlit**\n",
    "* ğŸ¤– Add **agents**\n",
    "* ğŸ§  Add **memory**\n",
    "* ğŸ” Swap models\n",
    "* â˜ Deploy safely\n",
    "\n",
    "This mirrors **real production LLM repos**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Before moving forward (small checkpoint)\n",
    "\n",
    "Next logical steps:\n",
    "\n",
    "1. Create this folder structure locally\n",
    "2. Add `.env` + API key\n",
    "3. Install dependencies\n",
    "4. Run `basic_chat.py`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3c734",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3972c1fe",
   "metadata": {},
   "source": [
    "\n",
    "Think of the **`app/` folder** as the **brain + control room** of your project.\n",
    "\n",
    "---\n",
    "\n",
    "## Big Picture First (very important)\n",
    "\n",
    "Imagine youâ€™re building a **factory** ğŸ­\n",
    "\n",
    "* Raw materials come in\n",
    "* Machines do the work\n",
    "* Output goes out\n",
    "\n",
    "In our case:\n",
    "\n",
    "* **Input** â†’ user text\n",
    "* **Machines** â†’ LLM (Groq)\n",
    "* **Output** â†’ AI response\n",
    "\n",
    "The `app/` folder is where **all the machines and rules live**.\n",
    "\n",
    "```\n",
    "app/\n",
    "â”œâ”€â”€ config.py\n",
    "â”œâ”€â”€ client.py\n",
    "â”œâ”€â”€ prompts.py\n",
    "â””â”€â”€ chat.py\n",
    "```\n",
    "\n",
    "Each file has **one clear responsibility**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Why do we even have an `app/` folder?\n",
    "\n",
    "### Simple answer:\n",
    "\n",
    "To **separate logic from experiments**.\n",
    "\n",
    "* `app/` â†’ *How things work*\n",
    "* `experiments/` â†’ *Trying things out*\n",
    "\n",
    "This separation:\n",
    "\n",
    "* Prevents messy code\n",
    "* Makes debugging easier\n",
    "* Is how real companies structure projects\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ `config.py` â€” the â€œSecurity Guard & Settings Roomâ€\n",
    "\n",
    "### What problem does it solve?\n",
    "\n",
    "Your project needs **secret information** (API keys).\n",
    "\n",
    "âŒ Bad practice:\n",
    "\n",
    "```python\n",
    "api_key = \"sk-123456\"\n",
    "```\n",
    "\n",
    "âœ” Good practice:\n",
    "\n",
    "```python\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "```\n",
    "\n",
    "### What `config.py` does:\n",
    "\n",
    "* Loads environment variables from `.env`\n",
    "* Checks if the API key exists\n",
    "* Stops the app early if something is wrong\n",
    "\n",
    "### Mental model:\n",
    "\n",
    "ğŸ›¡ **Security guard**\n",
    "\n",
    "> â€œYou are not allowed in unless you have the API key.â€\n",
    "\n",
    "This avoids silent failures later.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ `client.py` â€” the â€œConnection Managerâ€\n",
    "\n",
    "### What problem does it solve?\n",
    "\n",
    "You donâ€™t want to create Groq connections **everywhere** in your code.\n",
    "\n",
    "### What this file does:\n",
    "\n",
    "* Creates the Groq client **once**\n",
    "* Returns it whenever needed\n",
    "\n",
    "```python\n",
    "def get_groq_client():\n",
    "    return Groq(api_key=GROQ_API_KEY)\n",
    "```\n",
    "\n",
    "### Mental model:\n",
    "\n",
    "ğŸ“ **Telephone exchange**\n",
    "\n",
    "> â€œIf anyone wants to call Groq, use THIS phone.â€\n",
    "\n",
    "Why this matters later:\n",
    "\n",
    "* Easy to switch to OpenAI\n",
    "* Easy to add logging\n",
    "* Easy to add retries\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ `prompts.py` â€” the â€œInstruction Manualâ€\n",
    "\n",
    "LLMs donâ€™t just answer â€” they **follow instructions**.\n",
    "\n",
    "### What problem does it solve?\n",
    "\n",
    "Without structure:\n",
    "\n",
    "* Prompts become messy\n",
    "* You repeat the same system messages everywhere\n",
    "\n",
    "### What this file does:\n",
    "\n",
    "* Stores **system prompts**\n",
    "* Stores **prompt templates**\n",
    "\n",
    "```python\n",
    "SYSTEM_ASSISTANT = \"You are a helpful and concise AI assistant.\"\n",
    "```\n",
    "\n",
    "### Mental model:\n",
    "\n",
    "ğŸ“˜ **Company rulebook**\n",
    "\n",
    "> â€œThis is how the AI should behave.â€\n",
    "\n",
    "Later this becomes:\n",
    "\n",
    "* Agent roles\n",
    "* Tool-using instructions\n",
    "* Guardrails\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ `chat.py` â€” the â€œWorker Who Gets Things Doneâ€\n",
    "\n",
    "This is where **real work happens**.\n",
    "\n",
    "### What problem does it solve?\n",
    "\n",
    "You want **one clean function** that:\n",
    "\n",
    "* Takes user input\n",
    "* Calls the LLM\n",
    "* Returns the answer\n",
    "\n",
    "```python\n",
    "def chat_completion(user_input):\n",
    "    ...\n",
    "    return response\n",
    "```\n",
    "\n",
    "### Mental model:\n",
    "\n",
    "ğŸ§‘â€ğŸ’¼ **Skilled employee**\n",
    "\n",
    "> â€œGive me a task, Iâ€™ll handle Groq and bring back the result.â€\n",
    "\n",
    "This keeps your experiments **simple and readable**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ How everything connects (important)\n",
    "\n",
    "Letâ€™s trace ONE request:\n",
    "\n",
    "```\n",
    "experiments/basic_chat.py\n",
    "        â†“\n",
    "chat.py  â†’ prompts.py\n",
    "        â†“\n",
    "client.py\n",
    "        â†“\n",
    "config.py â†’ .env\n",
    "        â†“\n",
    "Groq API\n",
    "```\n",
    "\n",
    "Each file does **only one job** â€” this is clean software design.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Why this matters for your future learning\n",
    "\n",
    "This structure prepares you for:\n",
    "\n",
    "* âœ… Streamlit apps\n",
    "* âœ… Agent frameworks\n",
    "* âœ… LangGraph\n",
    "* âœ… Production APIs\n",
    "* âœ… Interviews (this impresses)\n",
    "\n",
    "Youâ€™re not â€œjust calling an APIâ€ â€”\n",
    "youâ€™re learning **how real AI systems are built**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c52cf",
   "metadata": {},
   "source": [
    "## Q - We will proceed but before that i  need to clear one thing , did we need to create a client for every LLM api use for example like Gemni Api , open Ai APi ? What does this clients resembels ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc517c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Short, direct answer first\n",
    "\n",
    "ğŸ‘‰ **Yes, almost every LLM provider has a â€œclientâ€ concept**\n",
    "ğŸ‘‰ **No, the client is not the LLM itself**\n",
    "ğŸ‘‰ **The client is just a communication wrapper**\n",
    "\n",
    "Now letâ€™s really understand *why*.\n",
    "\n",
    "---\n",
    "\n",
    "## What is an LLM â€œclientâ€ actually?\n",
    "\n",
    "Think of an LLM API as a **remote service on the internet**.\n",
    "\n",
    "You **cannot talk to it directly** like a local Python function.\n",
    "\n",
    "So providers give you a **client** â€” which is basically:\n",
    "\n",
    "> ğŸ“¦ A **pre-built helper** that knows\n",
    "> how to talk to the LLM server correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-world analogy (very important)\n",
    "\n",
    "### Imagine ordering food ğŸ”\n",
    "\n",
    "* **Restaurant** â†’ LLM (Groq, OpenAI, Gemini)\n",
    "* **Kitchen** â†’ Model (LLaMA, GPT-4, Gemini Pro)\n",
    "* **Waiter** â†’ Client\n",
    "* **Menu format** â†’ API schema\n",
    "* **Your order** â†’ Prompt\n",
    "\n",
    "You *could* walk into the kitchen yourself (raw HTTP calls),\n",
    "but the **waiter (client)**:\n",
    "\n",
    "* Knows the menu\n",
    "* Knows how to talk to the kitchen\n",
    "* Brings food back cleanly\n",
    "\n",
    "---\n",
    "\n",
    "## Do all LLM APIs have clients?\n",
    "\n",
    "Yes â€” but they may look slightly different.\n",
    "\n",
    "### ğŸ”¹ OpenAI\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "```\n",
    "\n",
    "### ğŸ”¹ Groq\n",
    "\n",
    "```python\n",
    "from groq import Groq\n",
    "client = Groq()\n",
    "```\n",
    "\n",
    "### ğŸ”¹ Google Gemini\n",
    "\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"...\")\n",
    "```\n",
    "\n",
    "ğŸ‘‰ Different syntax\n",
    "ğŸ‘‰ Same **concept**\n",
    "\n",
    "---\n",
    "\n",
    "## What does a client *really* do internally?\n",
    "\n",
    "A client usually handles:\n",
    "\n",
    "1. ğŸ” Authentication (API key)\n",
    "2. ğŸŒ HTTP requests\n",
    "3. ğŸ“¦ Request formatting (JSON)\n",
    "4. ğŸ“© Response parsing\n",
    "5. ğŸ” Retries & errors\n",
    "6. â± Timeouts\n",
    "\n",
    "Instead of *you* writing all this:\n",
    "\n",
    "```python\n",
    "requests.post(\n",
    "  url=\"https://api.llm.com/v1/chat\",\n",
    "  headers={...},\n",
    "  json={...}\n",
    ")\n",
    "```\n",
    "\n",
    "You just write:\n",
    "\n",
    "```python\n",
    "client.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "Cleaner. Safer. Faster.\n",
    "\n",
    "---\n",
    "\n",
    "## Do we *have* to use a client?\n",
    "\n",
    "### âŒ No (technically)\n",
    "\n",
    "You can always use raw HTTP calls.\n",
    "\n",
    "### âœ… Yes (professionally)\n",
    "\n",
    "Clients:\n",
    "\n",
    "* Reduce bugs\n",
    "* Follow API updates\n",
    "* Are expected in production code\n",
    "\n",
    "In real companies, **raw calls are rare** unless youâ€™re building SDKs.\n",
    "\n",
    "---\n",
    "\n",
    "## Why we created `client.py` in *our* project\n",
    "\n",
    "Important distinction ğŸ‘‡\n",
    "There are **two kinds of â€œclientsâ€ here**:\n",
    "\n",
    "### 1ï¸âƒ£ Provider client (Groq / OpenAI)\n",
    "\n",
    "```python\n",
    "Groq()\n",
    "OpenAI()\n",
    "```\n",
    "\n",
    "Provided by the company.\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Your application client** (this is what we created)\n",
    "\n",
    "```python\n",
    "def get_groq_client():\n",
    "    return Groq(api_key=...)\n",
    "```\n",
    "\n",
    "This is:\n",
    "\n",
    "* Your abstraction\n",
    "* Your control layer\n",
    "* Your future-proofing\n",
    "\n",
    "---\n",
    "\n",
    "## Why this abstraction is powerful\n",
    "\n",
    "Tomorrow you can do this:\n",
    "\n",
    "```python\n",
    "if provider == \"groq\":\n",
    "    return Groq(...)\n",
    "elif provider == \"openai\":\n",
    "    return OpenAI(...)\n",
    "```\n",
    "\n",
    "And **nothing else in your app breaks**.\n",
    "\n",
    "This is how:\n",
    "\n",
    "* LangChain\n",
    "* LangGraph\n",
    "* CrewAI\n",
    "  are built internally.\n",
    "\n",
    "---\n",
    "\n",
    "## Mental model to remember (lock this in ğŸ§ )\n",
    "\n",
    "> â The client is NOT the brain\n",
    "> It is the phone that calls the brain â\n",
    "\n",
    "* LLM = brain\n",
    "* Client = phone\n",
    "* Prompt = message\n",
    "* Response = reply\n",
    "\n",
    "---\n",
    "\n",
    "## Final clarity check (quick Q&A)\n",
    "\n",
    "**Q: Do I create a new client for every request?**\n",
    "ğŸ‘‰ No. Usually **one client reused**.\n",
    "\n",
    "**Q: Do all LLMs need clients?**\n",
    "ğŸ‘‰ Yes, unless you do raw HTTP.\n",
    "\n",
    "**Q: Is client creation expensive?**\n",
    "ğŸ‘‰ No, but reuse is cleaner.\n",
    "\n",
    "**Q: Will this help in agents & Streamlit?**\n",
    "ğŸ‘‰ Absolutely. This is foundational.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322bfe36",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
